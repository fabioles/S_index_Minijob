{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routines adapted from git project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import filepaths\n",
    "\n",
    "def clean_objectlists(objectlist,path_to_outfile):\n",
    "    '''\n",
    "\n",
    "    :param objectlist: list with object names\n",
    "    :param path_to_outfile: pathname where list should be stored\n",
    "    :return: list with object names according to SIMBAD\n",
    "    '''\n",
    "    objectlist_clean = []\n",
    "    for source in objectlist:\n",
    "        if source[:2] == 'Cl' or source[:2] == 'EM':\n",
    "            obj_class = source[:2]\n",
    "            source_clean = obj_class+'* '+source[2:]\n",
    "        elif source[:2] == 'EQ' :\n",
    "            source_clean = ''\n",
    "        elif source == 'FHya':\n",
    "            source_clean = 'F Hya'\n",
    "        elif source == 'f02cyg':\n",
    "            source_clean = '*f02 Cyg'\n",
    "        elif source == 'fPer':\n",
    "            source_clean = 'f Per'\n",
    "        elif source == 'GJ3404A':\n",
    "            source_clean = 'GJ3404'\n",
    "        elif source == 'GJ3193B':\n",
    "            source_clean = 'GJ3193'        \n",
    "        elif source[:2] == 'HD':\n",
    "            if source[-1].isalpha():\n",
    "                source_clean = source[:-1]\n",
    "            else:\n",
    "                source_clean = source\n",
    "        elif source == 'KOCH':\n",
    "            source_clean = ''\n",
    "        elif source[:3]=='NGC':\n",
    "            source_clean = source[:7]+' '+source[7:]\n",
    "        elif source[:4] == 'NOVA':\n",
    "            source_clean = source[:4] + ' ' + source[4:]\n",
    "        elif source[0]=='V' and source != 'VB10' and source != 'VAnd' and source != 'VepsEri' and source[1] != '*':\n",
    "            source_clean = 'V*'+source[1:]\n",
    "        elif source =='VAnd':\n",
    "            source_clean = 'V And'\n",
    "        elif source == 'VepsEri':\n",
    "            source_clean = 'V*eps Eri'\n",
    "        elif source[0].islower() and source[:4] != 'kelt':\n",
    "            source_clean= '*'+source\n",
    "        elif source[:3]=='ADS':\n",
    "            source_clean = ''\n",
    "        elif source =='Sol':\n",
    "            source_clean = ''\n",
    "        elif source == 'CD_44836B':\n",
    "            source_clean == ''\n",
    "        elif source == 'CD_44836A':\n",
    "            source_clean == ''\n",
    "        # Noch nicht bekannt: Form \"Cl* Melotte 25 VA AGe m\"\n",
    "        else:\n",
    "            source_clean = source\n",
    "        objectlist_clean.append(source_clean.strip())\n",
    "    object_frame = pd.DataFrame({'object':objectlist,'object_simbad':objectlist_clean})\n",
    "    object_frame.to_csv(path_to_outfile,index=False)\n",
    "    return objectlist_clean\n",
    "\n",
    "def generate_objectlists_for_gaia(objectlist,folder,extra_name=''):\n",
    "    '''\n",
    "    Generates objectlists as chunks of 200 objects in order to downlad gaia data from http://gaia.ari.uni-heidelberg.de/singlesource.html\n",
    "    (since the maximum number of single sources is 200)\n",
    "    :param objectlist: list with single sources\n",
    "    :param folder: folder where the lists should be generated\n",
    "    :return: Nothing, lists are directly generated as files in the folder\n",
    "    '''\n",
    "    num_of_lists = int(np.ceil(len(objectlist)/200))\n",
    "    for i in range(num_of_lists-1):\n",
    "        with open(folder+f'sources_{extra_name}_{i}.csv','w+') as f:\n",
    "            f.writelines('\\n'.join(objectlist[i*200:i*200+200]))\n",
    "    with open(folder+f'sources_{extra_name}_{num_of_lists-1}.csv', 'w+') as f:\n",
    "        f.writelines('\\n'.join(objectlist[(num_of_lists-1) * 200:]))\n",
    "    return\n",
    "\n",
    "def generate_gaia_frame(folder,columns ='../column_names_subset.txt',sourcefile_pattern = None):\n",
    "    '''\n",
    "    Generates a frame with gaia data from all sources lists, sources lists should be in the the form 'SingleSource*'\n",
    "    :param folder: folder where the single gaia tables are\n",
    "    :param sourcefile_pattern: pattern which is looked for in the singlesource files. If None, every file with pattern 'SingleSource' is taken\n",
    "    :param columns: either str or list, gives the columns subset. If str, the parameter is interpreted as filename,\n",
    "    which is looked for in folder\n",
    "    :return: pandas  DataFrame with the gaia data for all downloaded single sources\n",
    "    '''\n",
    "    origin = os.getcwd()\n",
    "    os.chdir(folder)\n",
    "    if sourcefile_pattern is not None:\n",
    "        file_list= glob('SingleSource*'+sourcefile_pattern+'*')\n",
    "    else:\n",
    "        file_list = glob('SingleSource*')\n",
    "    frames = []\n",
    "    for file in file_list:\n",
    "        frames.append(pd.read_csv(file))\n",
    "    gaia_frame = pd.concat(frames)\n",
    "    column_names = []\n",
    "    if type(columns)==str:\n",
    "        with open(columns) as f:\n",
    "            for line in f:\n",
    "                column_names.append(line.rstrip('\\n'))\n",
    "    else:\n",
    "        column_names = columns\n",
    "    os.chdir(origin)\n",
    "    return gaia_frame[column_names]\n",
    "\n",
    "\n",
    "def find_gaia_errors(gaia_frame):\n",
    "    \"\"\"\n",
    "    calculates errorbars from gaia percentiles\n",
    "    :param gaia_frame: data frame with gaia params\n",
    "    :return: the dataframe, enriched by the errorbars\n",
    "    \"\"\"\n",
    "    df = gaia_frame\n",
    "    df['teff_err_lower'] = df['teff_val'] - df['teff_percentile_lower']\n",
    "    df['teff_err_upper'] = df['teff_percentile_upper'] - df['teff_val']\n",
    "    df['lum_err_lower'] = df['lum_val'] - df['lum_percentile_lower']\n",
    "    df['lum_err_upper'] = df['lum_percentile_upper'] - df['lum_val']\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def create_database(polar_frame, gaia_frame):\n",
    "    '''\n",
    "    joins the polarbase data and the gaia data\n",
    "    :param polar_frame: the data from polarbase\n",
    "    :param gaia_frame: the respective gaia data for single sources\n",
    "    :return: pandas DataFrame with polarbase data where gaia data are available\n",
    "    '''\n",
    "    together_frame = pd.merge(polar_frame, gaia_frame, left_on='objet', right_on='input_position', how='outer',\n",
    "                              suffixes=('_polarbase', '_gaia'))\n",
    "    return together_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean catalog file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_catalog(path):\n",
    "    '''\n",
    "    loads the catalog from catalog.dat, which has space seperated table with some entries missing.\n",
    "    '''\n",
    "    catalog_list = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            Seq = int(line[:4])\n",
    "            Name = line[4:15].strip()\n",
    "            SpType = line[15:37].strip()\n",
    "            BV = line[37:51]\n",
    "            RAdeg = line[51:64]\n",
    "            DEdeg = line[64:77]\n",
    "            VMAG = line[77:93]\n",
    "            Plx = line[93:100]\n",
    "            Smean = line[100:118]\n",
    "            Smed = line[118:136]\n",
    "            logRpHK = line[136:-2]\n",
    "\n",
    "            #convert these entries to floats\n",
    "            line_list = [Seq, Name, SpType]\n",
    "            for parameter in [BV, RAdeg, DEdeg, VMAG, Plx, Smean, Smed, logRpHK]:\n",
    "                if parameter.strip() == '':\n",
    "                    parameter = None\n",
    "                else:\n",
    "                    parameter = float(parameter)\n",
    "                line_list.extend([parameter])\n",
    "            catalog_list.append(line_list)\n",
    "\n",
    "    catalog_dict = {'Seq': [entry[0] for entry in catalog_list],\n",
    "                   'Name': [entry[1] for entry in catalog_list],\n",
    "                   'SpType': [entry[2] for entry in catalog_list],\n",
    "                   'BV': [entry[3] for entry in catalog_list],\n",
    "                   'RAdeg': [entry[4] for entry in catalog_list],\n",
    "                   'DEdeg': [entry[5] for entry in catalog_list],\n",
    "                   'VMAG': [entry[6] for entry in catalog_list],\n",
    "                   'Plx': [entry[7] for entry in catalog_list],\n",
    "                   'Smean': [entry[8] for entry in catalog_list],\n",
    "                   'Smed': [entry[9] for entry in catalog_list],\n",
    "                   'logRpHK': [entry[10] for entry in catalog_list]}\n",
    "\n",
    "    catalog = pd.DataFrame(catalog_dict)\n",
    "    \n",
    "    return catalog\n",
    "\n",
    "def clean_catalog(catalog):\n",
    "    '''\n",
    "    Removes duplicate entries in catalog. The resulting catalog contains median values of parameters\n",
    "    if there were multiple entries for a star.\n",
    "    '''\n",
    "    new_cataloglist = []\n",
    "    parameter_list = ['Seq', 'Name', 'SpType', 'BV', 'RAdeg', 'DEdeg',\n",
    "                      'VMAG', 'Plx', 'Smean', 'Smed', 'logRpHK']\n",
    "    for name in set(catalog['Name']):\n",
    "        one_entry = []\n",
    "        ind = np.where(catalog['Name'] == name)[0]\n",
    "        if ind.size == 1:\n",
    "            #if there is only one entry, take all of these values\n",
    "            for parameter in parameter_list:\n",
    "                one_entry.extend([catalog[parameter][ind[0]]])\n",
    "        else:\n",
    "            #if there are multiple entries, take median of float parameters\n",
    "            for parameter in parameter_list[:3]:\n",
    "                one_entry.extend([catalog[parameter][ind[0]]])\n",
    "            for parameter in parameter_list[3:]:\n",
    "                ind_notnan = np.where(pd.notnull(catalog[parameter][ind]))[0]\n",
    "                if ind_notnan.size == 0:\n",
    "                     one_entry.extend([np.nan])\n",
    "                else:\n",
    "                    median_value = np.median(catalog[parameter][ind[ind_notnan]])\n",
    "                    one_entry.extend([median_value])\n",
    "        new_cataloglist.append(one_entry)\n",
    "\n",
    "    catalog_clean = pd.DataFrame(new_cataloglist, columns = ['Seq', 'Name', 'SpType', 'BV', 'RAdeg', 'DEdeg', 'VMAG', 'Plx', 'Smean', 'Smed', 'logRpHK'])\n",
    "    \n",
    "    return catalog_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and clean S-index catalog\n",
    "catalog_path = filepaths.base_path+'catalog_original.dat'\n",
    "catalog = load_catalog(catalog_path)\n",
    "catalog_clean = clean_catalog(catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 / unknown"
     ]
    }
   ],
   "source": [
    "#automatically request gaia data, excluding all stars not found in simbad\n",
    "notinSimbad = []\n",
    "for name in catalog_clean['Name']:\n",
    "    filename = filepaths.base_path+'GaiaDownload/'+name+'.csv'\n",
    "    wget.download('http://gaia.ari.uni-heidelberg.de/singlesource/search?obj='+name, out=filename)\n",
    "    dataframe = pd.read_csv(filename)\n",
    "    if dataframe.columns[0][:5] == 'ERROR':\n",
    "        notinSimbad.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save list of stars not in the simbad catalog\n",
    "pd.DataFrame(notinSimbad, columns = ['Name']).to_csv(filepaths.base_path+'NotInSimbad.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all single files into one large dataframe\n",
    "df_list = []\n",
    "for name in catalog_clean['Name']:\n",
    "    if not name in notinSimbad:\n",
    "        filename = filepaths.base_path+'CheckSource/'+name+'.csv'\n",
    "#         with open(filename) as file:\n",
    "#             print(file.read().split(','))\n",
    "        df = pd.read_csv(filename)\n",
    "        df.insert(0, 'Name', name)\n",
    "        df_list.append(df)\n",
    "df_combined = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find errors of gaia parameters\n",
    "gf = find_gaia_errors(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with s-index catalog and save\n",
    "data = pd.merge(gf, catalog_clean,on='Name')\n",
    "data.to_csv(filepaths.base_path+'CombinedCatalog.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
